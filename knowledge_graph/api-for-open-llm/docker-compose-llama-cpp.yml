version: '3.10'

services:
  llama-cpp-apiserver:
    image: llm-api:llama-cpp
    command: python3 api/server.py
    ulimits:
      stack: 67108864
      memlock: -1
    environment:
      - PORT=8000
      - MODEL_NAME=baichuan2
      - MODEL_PATH=checkpoints/baichuan2-7b-chat-gguf/baichuan2-7b-chat.Q3_K.gguf
      - ENGINE=llama.cpp
      - N_GPU_LAYERS=-1
    volumes:
      - $PWD:/workspace
      # model path need to be specified /workspaceif not in pwd
#      - /data/checkpoints:/workspace/checkpoints
    env_file:
      - .env.example
    ports:
      - "7891:8000"
    restart: always
    networks:
      - llama-cpp-apinet
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

networks:
  llama-cpp-apinet:
    driver: bridge
    name: llama-cpp-apinet